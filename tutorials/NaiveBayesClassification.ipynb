{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "First, we load the data from disk into a Dataframe.\n",
    "\n",
    "A Dataframe is essentially a table, or 2D-array/Matrix with a name for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phone_df = pandas.read_csv('phone-data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's preview the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the top 5 rows\n",
    "display(phone_df.head())\n",
    "# Summarize the data\n",
    "phone_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "We first select only the columns we are interested.\n",
    "\n",
    "For this example we will be training a model to predict the \"app tag\" given a \"Sentence Utterance\". Thus we will be only selecting these two. Others can also be selected, but these two will serve as an example.\n",
    "\n",
    "We call the DataFrame.describe() again.\n",
    "Notice that there are 24 unique labels/classes that the model will try to predict.\n",
    "But the \"#ERROR!\" class is not a class we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = phone_df[[\"Sentence Utterance\", \"app tag\"]]\n",
    "data_df.columns = ['input', 'label']\n",
    "display(data_df.describe())\n",
    "display(data_df.label.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Nulls and Error\n",
    "\n",
    "from above, we can see that there are 24 unique labels\n",
    "\n",
    "But we want to remove the \"#ERROR!\" which is means that the paricular row is un useful data\n",
    "\n",
    "We will clean unwanted data by removing Nulls and invalid values (in this case \"#ERROR!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Number for rows %d\" % (len(data_df)))\n",
    "data_df = data_df[data_df.input.notnull()]\n",
    "print(\"remove nulls in input\")\n",
    "print(\"Number for rows %d\" % (len(data_df)))\n",
    "data_df = data_df[data_df.label.notnull()]\n",
    "print(\"remove nulls in label\")\n",
    "print(\"Number for rows %d\" % (len(data_df)))\n",
    "data_df = data_df[data_df.label != \"#ERROR!\"]\n",
    "print(\"remove \\\"#ERROR!\\\" in label\")\n",
    "print(\"Number for rows %d\" % (len(data_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicates input\n",
    "\n",
    "There are some duplicates in the input of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, we no longer have \"#ERROR!\" in our labels\n",
    "\n",
    "But there are still duplicates in our input. 7 rows with \"สอบถามยอดค้างชำระค่ะ\"\n",
    "\n",
    "We will remove them now, by keeping only the first entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.drop_duplicates(\"input\", keep=\"first\")\n",
    "display(data_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substitute Strings in Label\n",
    "Computer don't actually understand the string in the label so we will substitute them with a number for each unique value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data_df.as_matrix(), copy=True)\n",
    "\n",
    "unique_label = data_df.label.unique()\n",
    "\n",
    "label_2_num_map = dict(zip(unique_label, range(len(unique_label))))\n",
    "num_2_label_map = dict(zip(range(len(unique_label)), unique_label))\n",
    "\n",
    "print(\"Create Mappings\")\n",
    "display(num_2_label_map)\n",
    "display(label_2_num_map)\n",
    "\n",
    "print(\"Before Mappings\")\n",
    "display(data[:, 1])\n",
    "data[:,1] = np.vectorize(label_2_num_map.get)(data[:,1])\n",
    "\n",
    "print(\"After Mappings\")\n",
    "display(data[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String cleaning\n",
    "Trim whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_str(string):\n",
    "    return string.strip()\n",
    "     \n",
    "# Trim of extra begining and trailing whitespace in the string\n",
    "print(\"Before\")\n",
    "print(data)\n",
    "data[:,0] = np.vectorize(strip_str)(data[:,0])\n",
    "print(\"After\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Class Count\n",
    "\n",
    "We will now visualize the class imbalance. Note that training directly on imbalance dataset can yield bad results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(label, count):\n",
    "    fig, ax = plt.subplots()\n",
    "    ind = np.arange(len(count))\n",
    "    rects1 = ax.bar(ind, count, 0.5)\n",
    "\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Count for each class')\n",
    "    ax.set_xticks(ind)\n",
    "    ax.set_xticklabels(label)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "label, count = np.unique(data[:, 1], return_counts=True)\n",
    "plot(label, count)\n",
    "\n",
    "# pack the label and count together\n",
    "bundle = list(zip(label, count))\n",
    "# sort them by count\n",
    "bundle = sorted(bundle, key=lambda e: e[1], reverse=True) \n",
    "# unpack the values\n",
    "label, count = zip(*bundle)\n",
    "plot(label, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our training data with input and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Which is just a fancy word for making the input work with our model.\n",
    "\n",
    "The models that we are going to tackle do not accpet varying size input, so we have to transform our input in some ways that makes the input have this property while also retaining some useful information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature #1: Char count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the Chars\n",
    "We will first find the list of possible chars in the dataset, we can just Google all the possible chars in Thai and English or we can just obtain it from the data set. The code bellow will do the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_the_string = \"\".join(data[:, 0])\n",
    "\n",
    "np_str = np.array(list(all_the_string))\n",
    "all_char = np.unique(np_str)\n",
    "\n",
    "sorted(all_char)\n",
    "print(\"There are %d unique chars in the data set\" % len(all_char))\n",
    "print(all_char)\n",
    "char_map = dict(zip(all_char, range(len(all_char))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_str(string):\n",
    "    global all_char, char_map\n",
    "    result = np.zeros(len(all_char))\n",
    "    np_str = np.array(list(string))\n",
    "    str_char, str_char_count = np.unique(np_str, return_counts=True)\n",
    "    for char, count in zip(str_char, str_char_count):\n",
    "        result[char_map[char]] = count\n",
    "    return result\n",
    "\n",
    "# run example feature transformation\n",
    "print(\"Example String to feature conversion\")\n",
    "display(data[0, 0])\n",
    "display(count_str(data[0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on data set\n",
    "temp = np.vectorize(count_str, otypes=[object])(data[:, 0])\n",
    "x_f1 = np.array([[e for e in sl] for sl in temp.tolist()])\n",
    "label = data[:, 1]\n",
    "print(\"Data\")\n",
    "print(\"Data shape\", x_f1.shape)\n",
    "print(\"label shape\", label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature #2: Keyword Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code bellow will show the first 3 entries for each class\n",
    "\n",
    "Use this to find some keywords that you believe will useful for the classifer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show_first_in_label(first, select_label):\n",
    "    print(\"Showing label \\\"%s\\\"\" % num_2_label_map[select_label])\n",
    "    select = data[data[:, 1] == select_label, 0]\n",
    "    for i in range(min(first, len(select))):\n",
    "        print(i, select[i])\n",
    "    print(\"\")\n",
    "        \n",
    "first_three = 3\n",
    "number_of_classes = 23\n",
    "for i in range(number_of_classes):\n",
    "    show_first_in_label(first_three, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the entries used to find keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 30\n",
    "display(data[index, 0], num_2_label_map[data[index, 1]])\n",
    "\n",
    "index = 40\n",
    "display(data[index, 0], num_2_label_map[data[index, 1]])\n",
    "\n",
    "index = 80\n",
    "display(data[index, 0], num_2_label_map[data[index, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add keywords here, some are already added for you as an example. \n",
    "The transformed features (keywords) should differentiate each classes from one another. See bellow that the 3 entries each from 3 classes can be differentiate using the keywords added as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords = [\"โปร\", \"โทร\", \"ไม่ได้\", \"iservice\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"has_keyword\" function only detects the keyword. But you can also experiment with counting the occurrence of the keyword by modifying the function bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_keyword(string):\n",
    "    global keywords\n",
    "    result = np.zeros(len(keywords))\n",
    "    for index, keyword in enumerate(keywords):\n",
    "        if keyword in string:\n",
    "            result[index] = 1\n",
    "    return result\n",
    "\n",
    "def preview(string_ind):\n",
    "    print(\"Entry\")\n",
    "    display(data[string_ind, 0])\n",
    "    print(\"Feature\")\n",
    "    print(has_keyword(data[string_ind, 0]), \"->\", num_2_label_map[data[string_ind, 1]])\n",
    "    print(\"\")\n",
    "\n",
    "# run example feature transformation\n",
    "print(\"Example String to feature conversion\\n\\n\")\n",
    "preview(30)\n",
    "preview(40)\n",
    "preview(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on data set\n",
    "temp = np.vectorize(has_keyword, otypes=[object])(data[:, 0])\n",
    "x_f2 = np.array([[e for e in sl] for sl in temp.tolist()])\n",
    "label = data[:, 1].astype(int)\n",
    "print(\"Data\")\n",
    "print(\"Data shape\", x_f2.shape)\n",
    "print(\"label shape\", label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "See how well the model can fit to our current data. This is a quick and dirty way of testing hand crafted features. The model will not generalize if it does not fit in the first place. We (you) will do proper training later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(x_f1, label)\n",
    "y_pred = model.predict(x_f1)\n",
    "print(\"Model Acc. on train data %f%%\"\n",
    "       % ((label == y_pred).sum() / x_f1.shape[0] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(x_f2, label)\n",
    "y_pred = model.predict(x_f2)\n",
    "print(\"Model Acc. on train data %f%%\"\n",
    "       % ((label == y_pred).sum() / x_f1.shape[0] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "#### Feature #1\n",
    "##### Split  data into train-data test-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display model performance\n",
    "Accuracy, confusion-matrix, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature #2\n",
    "##### Split  data into train-data test-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Display model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try combining the 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
